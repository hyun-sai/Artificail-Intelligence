{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP80Ao3tDntttf1kwKYXhK+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyun-sai/Artificail-Intelligence/blob/main/%EB%B6%84%EB%A5%98%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJZejDznqd3y",
        "outputId": "ccf6dcb5-00d8-48b6-886a-78e75799f612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7118644067796611\n"
          ]
        }
      ],
      "source": [
        "from re import X\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nbastat=pd.read_csv(\"nbastat2022c.csv\")\n",
        "m=len(nbastat)\n",
        "X=nbastat[['REB','AST','STL','BLK']]\n",
        "Y=nbastat['Position'].apply(lambda x:1 if x=='C' else 0)\n",
        "X=X.dropna()\n",
        "Y=Y.dropna()\n",
        "X=(np.array(X)).reshape(m,4)\n",
        "Y=(np.array(Y)).reshape(m,1)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=1004)\n",
        "class LogisticRegression:\n",
        "    def __init__(self,lr=0.1,epochs=10000):\n",
        "        self.lr=lr\n",
        "        self.epochs=epochs\n",
        "        self.theta=None\n",
        "    def sigmoid(self,z):\n",
        "        return 1/(1+np.exp(-z))\n",
        "    def compute_loss(self,Y,Y_hat):\n",
        "        m=len(Y)\n",
        "        return -1/m*np.sum(Y*np.log(Y_hat+1e-8)+(1-Y)*np.log(1-Y_hat+1e-8))\n",
        "    def fit(self,X,Y):\n",
        "        m,n=X.shape\n",
        "        X=np.hstack((np.ones((m, 1)), X))\n",
        "        self.theta=np.zeros(n+1).reshape(n+1,1)\n",
        "        losses=[]\n",
        "        for i in range(self.epochs):\n",
        "            z=np.dot(X,self.theta)\n",
        "            Y_hat=self.sigmoid(z)\n",
        "            gradient=(1/m)*X.T.dot(Y_hat-Y) #gradient의 크기를 지정을 안해줌 세타는 해줬는데 회귀에서도\n",
        "            self.theta=self.theta-self.lr*gradient\n",
        "            loss=self.compute_loss(Y,Y_hat)\n",
        "            losses.append(loss)\n",
        "        return losses\n",
        "    def predict_probability(self,X): #Y_hat 출력\n",
        "        m=X.shape[0]\n",
        "        X=np.hstack((np.ones((m, 1)), X))\n",
        "        return self.sigmoid(np.dot(X,self.theta))\n",
        "\n",
        "    def predict(self,X,threshold=0.5):\n",
        "        probability=self.predict_probability(X)\n",
        "        return (probability>=threshold).astype(int)\n",
        "model = LogisticRegression(lr=0.5, epochs=10000)\n",
        "losses = model.fit(X_train, Y_train)\n",
        "Y_train_hat = model.predict(X_train)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Y_train, Y_train_hat)\n",
        "tn,fp,fn,tp=cm.ravel()\n",
        "precision=tp/(tp+fp)\n",
        "recall=tp/(tp+fn)\n",
        "f1_score=(2*precision*recall)/(precision+recall)\n",
        "print(f1_score)\n",
        "야 짜증나게 하지마\n",
        "\n"
      ]
    }
  ]
}